import torch
import matplotlib.pyplot as plt
from d2l import torch as d2l
import os

# 设置环境变量以解决 OpenMP 冲突问题
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
plt.show()

y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
plt.show()

y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
plt.show()

# Clear out previous gradients
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
plt.show()


""" Exercises
1. 线性组合的线性组合仍是线性函数
2. 简单求导
3. ReLU 将输入分成非负和负两部分，每一部分的输出都是线性的。通过多层 ReLU，可以得到不同输入区间的分段线性函数。
4. 因为 tanh 和 sigmoid 都可以近似任何平滑的非线性函数，在加上偏置项后，它们的表示能力是相同的。
5. 以下是GPT-4o的回答：
     
    **批归一化（Batch Normalization）** 是一种神经网络训练技术，它在每个小批次上对数据进行归一化，
    使得网络的输入分布更稳定，从而加速训练。具体来说，批归一化会将每个小批次的输入重新调整，使其均值为 0、方差为 1。
    然而，这种方法可能会带来一些问题，尤其是以下两点：
    1. **批次间的数据分布差异**：批归一化是在每个小批次（mini-batch）上独立应用的，
    这意味着不同小批次的均值和方差可能不同。特别是在训练阶段，不同批次的数据分布可能会有所不同，
    导致模型的输出在每个批次之间产生波动。这种波动可能会让模型的学习变得不稳定，特别是当批次大小较小时，这种波动会更显著。
    2. **训练和测试分布不同的问题**：在训练阶段，批归一化的均值和方差是根据训练数据的批次来计算的。
    然而，在测试阶段，模型使用的是训练阶段的总体均值和方差，而不是单个批次的均值和方差。
    这种不一致可能会导致模型的泛化性能下降，特别是在测试数据分布和训练数据分布不完全一致的情况下。
    总之，批归一化在小批次上计算均值和方差，这种分布的波动可能会影响模型在新数据上的表现（泛化性能）。
    
    批归一化（Batch Normalization）这样操作的原因主要有以下几个：
    1. **稳定数据分布**：在训练神经网络时，不同层的输入数据分布可能会随训练进程变化，
    导致网络难以收敛。这种现象称为**内部协变量偏移**。批归一化通过将每个小批次的输入标准化，
    使其均值为 0，方差为 1，从而稳定数据分布，减少这种偏移，帮助模型更快地学习到合适的参数。
    2. **加速训练**：由于批归一化减少了数据分布的波动，优化算法在训练时可以使用更大的学习率，
    从而加速训练过程。同时，批归一化也能够减少梯度的爆炸和消失现象，使得梯度更加平稳。
    3. **减少过拟合**：批归一化在每个小批次上独立应用，这种在小批次内的归一化操作引入了一些噪声，
    相当于一种正则化效果，能够一定程度上抑制模型的过拟合。虽然它不是传统的正则化方法，但在训练过程中表现出类似的效果。
    4. **适应非线性激活**：在每一层激活函数之前对数据进行归一化，可以使激活函数的输入保持在相对稳定的范围内。
    这样，非线性激活（如 ReLU、Sigmoid 等）可以更有效地工作，减少出现梯度消失或梯度爆炸的风险。
    综上所述，批归一化通过对小批次的数据进行归一化，帮助稳定网络的训练过程，提高模型的训练效率，并增强泛化能力。
    
6. 当输入绝对值较大，sigmoid 的导数接近于 0，导致梯度消失。精度问题。
"""