{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T10:26:05.078693Z",
     "start_time": "2024-11-04T10:26:03.024439Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f2a1c68-4e59-4193-890f-4de46810f5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape # 其实是 net.__call__(X).shape 的缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cc9c116-97a0-4358-97ab-520088e06e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform 调用父类 nn.Mudule 的构造函数\n",
    "        # the necessary initialization 进行必要的初始化\n",
    "        super().__init__() # 调用父类\n",
    "        self.hidden = nn.LazyLinear(256)  # 隐藏层，输出特征数为 256\n",
    "        self.out = nn.LazyLinear(10)      # 输出层，输出特征数为 10\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the 定义模型的前向传播\n",
    "    # required model output based on the input X 即如何基于输入 X 返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X))) # F.relu 是 PyTorch 中的激活函数，用于将所有负值置为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9759ac-81cf-4b40-8497-bf01278711f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce5541a3-ace4-453b-aa70-41b062b9ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "            # 使用 nn.Module 类中的 add_module 方法将每个模块添加进来\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            # self.children() 是 PyTorch 的一个方法，它返回当前模块（在这里是 MySequential 实例）中所有直接子模块的迭代器。\n",
    "            # for module in self.children(): 会遍历所有子模块，并按顺序对每个子模块执行前向传播。\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b19e2d7-5cbf-4929-879e-85207804d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): LazyLinear(in_features=0, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "print(net)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5273e2f1-6499-4e4d-b215-3cb9946519e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 随机权重参数，在训练过程中不会计算梯度并保持不变\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X) # 这里就已经用内置的一个方法初始化并赋值了线性变换的 W 和 b\n",
    "        X = F.relu(X @ self.rand_weight + 1) # 这个自定义的 FixedHiddenMLP 只是想展示一下如何使用常量\n",
    "        X = self.linear(X) # 重用全连接层。这相当于与两个全连接层共享参数（ w 和 b ）\n",
    "        while X.abs().sum() > 1: # 控制流\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "112ce9cb-a14f-4ff7-a36f-1a84fe4e62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedHiddenMLP(\n",
      "  (linear): LazyLinear(in_features=0, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0276, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3096f51c-9aad-4c0a-a643-2472d8f35666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1952, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52b3b279-8b17-478b-926f-317ce90d7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exercise\n",
    "1. 如果将模块存储在 Python 列表中而不是使用 nn.ModuleList，则这些模块不会被 nn.Module 类检测到，因此它们的参数不会被注册到模型中。结果是，当调用 model.parameters() 或 model.children() 等方法时，Python 列表中的模块参数不会被包含进去，这将导致这些模块的参数无法参与训练和自动微分过程\n",
    "\n",
    "2. \"\"\"\n",
    "class ParallelModule(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "\n",
    "    def forward(self, X):\n",
    "        out1 = self.net1(X)\n",
    "        out2 = self.net2(X)\n",
    "        return torch.cat((out1, out2), dim=1) \n",
    "\"\"\"\n",
    "3. \"\"\"\n",
    "def create_network(module_class, num_instances, *args, **kwargs):\n",
    "    modules = [module_class(*args, **kwargs) for _ in range(num_instances)]\n",
    "    return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c8df1-6d24-442e-9bcf-ebd5430223fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
